{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paek-injin/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 : \n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 단계\n",
    "## 하나의 에폭은 학습과 검증 단계 두 부분으로 구성됨\n",
    "## 학습 단계 : 학습용 데이터셋을 반복하고 최적의 매개변수로 수렴함\n",
    "## 검증 단계 : 모델 성능이 개선되고 있는지 확인하기 위해 테스트 데이터셋을 반복함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수\n",
    "## MSELoss : 회귀문제에서\n",
    "## NLLLoss : 분류문제에서\n",
    "## CrossEntropyLoss : LogSoftmax + NLLLoss \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "# 모델의 출력 로짓을 nn.CrossEntropyLoss에 전달하여 로짓을 정규화하고 예측 오류를 계산함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저\n",
    "## 최적화 : 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정\n",
    "## 최적화 알고리즘 : 이 과정이 수행되는 방식을 정의함\n",
    "## 모든 최적화 절차는 optimizer 객체에 캡슐화됨.\n",
    "## SGD, Adam, RMSProp 등의 다양한 옵티마이저 존재\n",
    "\n",
    "### 옵티마이저 공부 필요\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "## 학습 단계에서 최적화는 3단계로 이뤄짐\n",
    "# 1) optimizer.zero_grad() : 모델 매개변수의 변화도를 재설정함, \n",
    "    ## 기본적으로 변화도는 더해지기 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정\n",
    "# 2) loss.backwards() : 예측 손실을 역전파 -> pytorch는 각 매개변수에 대한 손실의 변화도를 저장함\n",
    "# 3) 변화도 계산 뒤 optimizer.step() : 역전파 단계에서 수집된 변화도로 매개변수를 조정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 구현\n",
    "## 최적화 코드를 반복하여 수행하는 train_loop와\n",
    "## 테스트 데이터로 모델의 성능을 측정하는 test_loop 정의\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 예측과 손실 계산\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # 역전파 ## 최적화 3단계 세트로 발생!!\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch & 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss : {loss:>7f}    [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():    # 변화도 추적 멈추기!\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\nAccuracy : {(100*correct):0.1f}%, Avg loss : {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------\n",
      "loss : 2.174798    [    0/60000]\n",
      "loss : 2.159283    [   64/60000]\n",
      "loss : 2.173306    [  128/60000]\n",
      "loss : 2.176456    [  192/60000]\n",
      "loss : 2.161710    [  512/60000]\n",
      "loss : 2.152644    [  576/60000]\n",
      "loss : 2.156478    [  640/60000]\n",
      "loss : 2.154219    [  704/60000]\n",
      "loss : 2.158785    [ 1024/60000]\n",
      "loss : 2.156526    [ 1088/60000]\n",
      "loss : 2.159743    [ 1152/60000]\n",
      "loss : 2.170592    [ 1216/60000]\n",
      "loss : 2.151095    [ 1536/60000]\n",
      "loss : 2.136799    [ 1600/60000]\n",
      "loss : 2.131881    [ 1664/60000]\n",
      "loss : 2.168006    [ 1728/60000]\n",
      "loss : 2.160715    [ 8192/60000]\n",
      "loss : 2.100732    [ 8256/60000]\n",
      "loss : 2.116729    [ 8320/60000]\n",
      "loss : 2.137813    [ 8384/60000]\n",
      "loss : 2.123846    [ 8704/60000]\n",
      "loss : 2.136239    [ 8768/60000]\n",
      "loss : 2.166329    [ 8832/60000]\n",
      "loss : 2.134322    [ 8896/60000]\n",
      "loss : 2.116143    [ 9216/60000]\n",
      "loss : 2.137441    [ 9280/60000]\n",
      "loss : 2.111784    [ 9344/60000]\n",
      "loss : 2.152424    [ 9408/60000]\n",
      "loss : 2.134560    [ 9728/60000]\n",
      "loss : 2.137887    [ 9792/60000]\n",
      "loss : 2.138407    [ 9856/60000]\n",
      "loss : 2.137913    [ 9920/60000]\n",
      "loss : 2.118459    [16384/60000]\n",
      "loss : 2.074104    [16448/60000]\n",
      "loss : 2.114496    [16512/60000]\n",
      "loss : 2.125538    [16576/60000]\n",
      "loss : 2.111437    [16896/60000]\n",
      "loss : 2.111544    [16960/60000]\n",
      "loss : 2.125320    [17024/60000]\n",
      "loss : 2.117455    [17088/60000]\n",
      "loss : 2.115761    [17408/60000]\n",
      "loss : 2.127835    [17472/60000]\n",
      "loss : 2.089201    [17536/60000]\n",
      "loss : 2.106516    [17600/60000]\n",
      "loss : 2.110464    [17920/60000]\n",
      "loss : 2.122596    [17984/60000]\n",
      "loss : 2.142311    [18048/60000]\n",
      "loss : 2.099825    [18112/60000]\n",
      "loss : 2.042080    [24576/60000]\n",
      "loss : 2.098630    [24640/60000]\n",
      "loss : 2.080925    [24704/60000]\n",
      "loss : 2.086465    [24768/60000]\n",
      "loss : 2.102881    [25088/60000]\n",
      "loss : 2.080880    [25152/60000]\n",
      "loss : 2.068226    [25216/60000]\n",
      "loss : 2.053941    [25280/60000]\n",
      "loss : 2.094062    [25600/60000]\n",
      "loss : 2.085835    [25664/60000]\n",
      "loss : 2.083770    [25728/60000]\n",
      "loss : 2.077465    [25792/60000]\n",
      "loss : 2.092579    [26112/60000]\n",
      "loss : 2.084304    [26176/60000]\n",
      "loss : 2.069796    [26240/60000]\n",
      "loss : 2.059501    [26304/60000]\n",
      "loss : 2.057563    [32768/60000]\n",
      "loss : 2.047625    [32832/60000]\n",
      "loss : 2.040997    [32896/60000]\n",
      "loss : 2.080054    [32960/60000]\n",
      "loss : 2.044809    [33280/60000]\n",
      "loss : 2.036695    [33344/60000]\n",
      "loss : 2.047533    [33408/60000]\n",
      "loss : 2.026385    [33472/60000]\n",
      "loss : 2.059263    [33792/60000]\n",
      "loss : 2.031323    [33856/60000]\n",
      "loss : 2.057132    [33920/60000]\n",
      "loss : 2.019620    [33984/60000]\n",
      "loss : 2.026435    [34304/60000]\n",
      "loss : 2.040689    [34368/60000]\n",
      "loss : 2.046041    [34432/60000]\n",
      "loss : 2.003144    [34496/60000]\n",
      "loss : 2.046173    [40960/60000]\n",
      "loss : 2.026205    [41024/60000]\n",
      "loss : 1.972103    [41088/60000]\n",
      "loss : 1.991100    [41152/60000]\n",
      "loss : 2.029013    [41472/60000]\n",
      "loss : 2.027187    [41536/60000]\n",
      "loss : 1.978030    [41600/60000]\n",
      "loss : 2.000805    [41664/60000]\n",
      "loss : 1.984173    [41984/60000]\n",
      "loss : 2.016208    [42048/60000]\n",
      "loss : 1.981084    [42112/60000]\n",
      "loss : 2.016528    [42176/60000]\n",
      "loss : 2.017495    [42496/60000]\n",
      "loss : 1.944795    [42560/60000]\n",
      "loss : 1.988711    [42624/60000]\n",
      "loss : 2.019094    [42688/60000]\n",
      "loss : 1.992091    [49152/60000]\n",
      "loss : 1.963479    [49216/60000]\n",
      "loss : 1.998921    [49280/60000]\n",
      "loss : 1.917572    [49344/60000]\n",
      "loss : 1.977887    [49664/60000]\n",
      "loss : 1.989220    [49728/60000]\n",
      "loss : 1.909420    [49792/60000]\n",
      "loss : 1.978673    [49856/60000]\n",
      "loss : 1.971864    [50176/60000]\n",
      "loss : 2.008792    [50240/60000]\n",
      "loss : 1.977677    [50304/60000]\n",
      "loss : 1.946449    [50368/60000]\n",
      "loss : 1.970128    [50688/60000]\n",
      "loss : 1.973682    [50752/60000]\n",
      "loss : 1.971376    [50816/60000]\n",
      "loss : 1.972032    [50880/60000]\n",
      "loss : 1.918396    [57344/60000]\n",
      "loss : 1.926471    [57408/60000]\n",
      "loss : 1.941884    [57472/60000]\n",
      "loss : 1.920957    [57536/60000]\n",
      "loss : 1.893991    [57856/60000]\n",
      "loss : 1.892344    [57920/60000]\n",
      "loss : 1.908963    [57984/60000]\n",
      "loss : 1.929427    [58048/60000]\n",
      "loss : 1.965532    [58368/60000]\n",
      "loss : 1.950445    [58432/60000]\n",
      "loss : 1.942412    [58496/60000]\n",
      "loss : 1.930586    [58560/60000]\n",
      "loss : 1.939878    [58880/60000]\n",
      "loss : 1.880686    [58944/60000]\n",
      "loss : 1.896989    [59008/60000]\n",
      "loss : 1.928111    [59072/60000]\n",
      "Test Error: \n",
      "Accuracy : 56.4%, Avg loss : 1.913052\n",
      "\n",
      "Epoch 2\n",
      "--------------------------\n",
      "loss : 1.938833    [    0/60000]\n",
      "loss : 1.878491    [   64/60000]\n",
      "loss : 1.908522    [  128/60000]\n",
      "loss : 1.954808    [  192/60000]\n",
      "loss : 1.887644    [  512/60000]\n",
      "loss : 1.884628    [  576/60000]\n",
      "loss : 1.873160    [  640/60000]\n",
      "loss : 1.870491    [  704/60000]\n",
      "loss : 1.897601    [ 1024/60000]\n",
      "loss : 1.913548    [ 1088/60000]\n",
      "loss : 1.896298    [ 1152/60000]\n",
      "loss : 1.941340    [ 1216/60000]\n",
      "loss : 1.855701    [ 1536/60000]\n",
      "loss : 1.859746    [ 1600/60000]\n",
      "loss : 1.831993    [ 1664/60000]\n",
      "loss : 1.913409    [ 1728/60000]\n",
      "loss : 1.911315    [ 8192/60000]\n",
      "loss : 1.790756    [ 8256/60000]\n",
      "loss : 1.840270    [ 8320/60000]\n",
      "loss : 1.878404    [ 8384/60000]\n",
      "loss : 1.822933    [ 8704/60000]\n",
      "loss : 1.856216    [ 8768/60000]\n",
      "loss : 1.920943    [ 8832/60000]\n",
      "loss : 1.870633    [ 8896/60000]\n",
      "loss : 1.815616    [ 9216/60000]\n",
      "loss : 1.833789    [ 9280/60000]\n",
      "loss : 1.807877    [ 9344/60000]\n",
      "loss : 1.886536    [ 9408/60000]\n",
      "loss : 1.844786    [ 9728/60000]\n",
      "loss : 1.886894    [ 9792/60000]\n",
      "loss : 1.851009    [ 9856/60000]\n",
      "loss : 1.884776    [ 9920/60000]\n",
      "loss : 1.812412    [16384/60000]\n",
      "loss : 1.742449    [16448/60000]\n",
      "loss : 1.793781    [16512/60000]\n",
      "loss : 1.833929    [16576/60000]\n",
      "loss : 1.813659    [16896/60000]\n",
      "loss : 1.820717    [16960/60000]\n",
      "loss : 1.840063    [17024/60000]\n",
      "loss : 1.852905    [17088/60000]\n",
      "loss : 1.824250    [17408/60000]\n",
      "loss : 1.845446    [17472/60000]\n",
      "loss : 1.775747    [17536/60000]\n",
      "loss : 1.824817    [17600/60000]\n",
      "loss : 1.822484    [17920/60000]\n",
      "loss : 1.839856    [17984/60000]\n",
      "loss : 1.888275    [18048/60000]\n",
      "loss : 1.775110    [18112/60000]\n",
      "loss : 1.693246    [24576/60000]\n",
      "loss : 1.783922    [24640/60000]\n",
      "loss : 1.780391    [24704/60000]\n",
      "loss : 1.795604    [24768/60000]\n",
      "loss : 1.808845    [25088/60000]\n",
      "loss : 1.764262    [25152/60000]\n",
      "loss : 1.706270    [25216/60000]\n",
      "loss : 1.701413    [25280/60000]\n",
      "loss : 1.750967    [25600/60000]\n",
      "loss : 1.767904    [25664/60000]\n",
      "loss : 1.773802    [25728/60000]\n",
      "loss : 1.755419    [25792/60000]\n",
      "loss : 1.776022    [26112/60000]\n",
      "loss : 1.789483    [26176/60000]\n",
      "loss : 1.743053    [26240/60000]\n",
      "loss : 1.708602    [26304/60000]\n",
      "loss : 1.746220    [32768/60000]\n",
      "loss : 1.704376    [32832/60000]\n",
      "loss : 1.703532    [32896/60000]\n",
      "loss : 1.769213    [32960/60000]\n",
      "loss : 1.683633    [33280/60000]\n",
      "loss : 1.659980    [33344/60000]\n",
      "loss : 1.699508    [33408/60000]\n",
      "loss : 1.687286    [33472/60000]\n",
      "loss : 1.721124    [33792/60000]\n",
      "loss : 1.671956    [33856/60000]\n",
      "loss : 1.741509    [33920/60000]\n",
      "loss : 1.669483    [33984/60000]\n",
      "loss : 1.666845    [34304/60000]\n",
      "loss : 1.733175    [34368/60000]\n",
      "loss : 1.691464    [34432/60000]\n",
      "loss : 1.618310    [34496/60000]\n",
      "loss : 1.719948    [40960/60000]\n",
      "loss : 1.681645    [41024/60000]\n",
      "loss : 1.574723    [41088/60000]\n",
      "loss : 1.619389    [41152/60000]\n",
      "loss : 1.719827    [41472/60000]\n",
      "loss : 1.671783    [41536/60000]\n",
      "loss : 1.627692    [41600/60000]\n",
      "loss : 1.642822    [41664/60000]\n",
      "loss : 1.629475    [41984/60000]\n",
      "loss : 1.684484    [42048/60000]\n",
      "loss : 1.593336    [42112/60000]\n",
      "loss : 1.676570    [42176/60000]\n",
      "loss : 1.679047    [42496/60000]\n",
      "loss : 1.522603    [42560/60000]\n",
      "loss : 1.613610    [42624/60000]\n",
      "loss : 1.686919    [42688/60000]\n",
      "loss : 1.651412    [49152/60000]\n",
      "loss : 1.585755    [49216/60000]\n",
      "loss : 1.648944    [49280/60000]\n",
      "loss : 1.499104    [49344/60000]\n",
      "loss : 1.604920    [49664/60000]\n",
      "loss : 1.629247    [49728/60000]\n",
      "loss : 1.490093    [49792/60000]\n",
      "loss : 1.625185    [49856/60000]\n",
      "loss : 1.613269    [50176/60000]\n",
      "loss : 1.686457    [50240/60000]\n",
      "loss : 1.630591    [50304/60000]\n",
      "loss : 1.569786    [50368/60000]\n",
      "loss : 1.594404    [50688/60000]\n",
      "loss : 1.594709    [50752/60000]\n",
      "loss : 1.583573    [50816/60000]\n",
      "loss : 1.606083    [50880/60000]\n",
      "loss : 1.552961    [57344/60000]\n",
      "loss : 1.528361    [57408/60000]\n",
      "loss : 1.601187    [57472/60000]\n",
      "loss : 1.563197    [57536/60000]\n",
      "loss : 1.531353    [57856/60000]\n",
      "loss : 1.484933    [57920/60000]\n",
      "loss : 1.542238    [57984/60000]\n",
      "loss : 1.567684    [58048/60000]\n",
      "loss : 1.635724    [58368/60000]\n",
      "loss : 1.600960    [58432/60000]\n",
      "loss : 1.565753    [58496/60000]\n",
      "loss : 1.569012    [58560/60000]\n",
      "loss : 1.608727    [58880/60000]\n",
      "loss : 1.482570    [58944/60000]\n",
      "loss : 1.499361    [59008/60000]\n",
      "loss : 1.552852    [59072/60000]\n",
      "Test Error: \n",
      "Accuracy : 61.4%, Avg loss : 1.549384\n",
      "\n",
      "Epoch 3\n",
      "--------------------------\n",
      "loss : 1.610459    [    0/60000]\n",
      "loss : 1.459419    [   64/60000]\n",
      "loss : 1.523277    [  128/60000]\n",
      "loss : 1.634370    [  192/60000]\n",
      "loss : 1.501304    [  512/60000]\n",
      "loss : 1.504421    [  576/60000]\n",
      "loss : 1.458091    [  640/60000]\n",
      "loss : 1.462313    [  704/60000]\n",
      "loss : 1.531782    [ 1024/60000]\n",
      "loss : 1.568567    [ 1088/60000]\n",
      "loss : 1.515399    [ 1152/60000]\n",
      "loss : 1.618330    [ 1216/60000]\n",
      "loss : 1.436250    [ 1536/60000]\n",
      "loss : 1.479650    [ 1600/60000]\n",
      "loss : 1.415724    [ 1664/60000]\n",
      "loss : 1.546801    [ 1728/60000]\n",
      "loss : 1.571486    [ 8192/60000]\n",
      "loss : 1.407939    [ 8256/60000]\n",
      "loss : 1.494516    [ 8320/60000]\n",
      "loss : 1.533428    [ 8384/60000]\n",
      "loss : 1.445981    [ 8704/60000]\n",
      "loss : 1.502707    [ 8768/60000]\n",
      "loss : 1.576222    [ 8832/60000]\n",
      "loss : 1.552495    [ 8896/60000]\n",
      "loss : 1.432742    [ 9216/60000]\n",
      "loss : 1.443015    [ 9280/60000]\n",
      "loss : 1.436016    [ 9344/60000]\n",
      "loss : 1.528967    [ 9408/60000]\n",
      "loss : 1.484879    [ 9728/60000]\n",
      "loss : 1.569678    [ 9792/60000]\n",
      "loss : 1.486243    [ 9856/60000]\n",
      "loss : 1.559793    [ 9920/60000]\n",
      "loss : 1.430904    [16384/60000]\n",
      "loss : 1.350328    [16448/60000]\n",
      "loss : 1.430551    [16512/60000]\n",
      "loss : 1.487234    [16576/60000]\n",
      "loss : 1.453768    [16896/60000]\n",
      "loss : 1.474471    [16960/60000]\n",
      "loss : 1.496852    [17024/60000]\n",
      "loss : 1.552562    [17088/60000]\n",
      "loss : 1.484934    [17408/60000]\n",
      "loss : 1.525764    [17472/60000]\n",
      "loss : 1.416622    [17536/60000]\n",
      "loss : 1.496894    [17600/60000]\n",
      "loss : 1.486990    [17920/60000]\n",
      "loss : 1.507584    [17984/60000]\n",
      "loss : 1.599974    [18048/60000]\n",
      "loss : 1.394188    [18112/60000]\n",
      "loss : 1.334694    [24576/60000]\n",
      "loss : 1.449231    [24640/60000]\n",
      "loss : 1.454958    [24704/60000]\n",
      "loss : 1.492097    [24768/60000]\n",
      "loss : 1.480648    [25088/60000]\n",
      "loss : 1.426492    [25152/60000]\n",
      "loss : 1.328931    [25216/60000]\n",
      "loss : 1.346323    [25280/60000]\n",
      "loss : 1.396004    [25600/60000]\n",
      "loss : 1.410977    [25664/60000]\n",
      "loss : 1.440020    [25728/60000]\n",
      "loss : 1.413327    [25792/60000]\n",
      "loss : 1.440699    [26112/60000]\n",
      "loss : 1.513077    [26176/60000]\n",
      "loss : 1.409859    [26240/60000]\n",
      "loss : 1.341997    [26304/60000]\n",
      "loss : 1.424576    [32768/60000]\n",
      "loss : 1.372160    [32832/60000]\n",
      "loss : 1.388668    [32896/60000]\n",
      "loss : 1.474674    [32960/60000]\n",
      "loss : 1.326254    [33280/60000]\n",
      "loss : 1.322330    [33344/60000]\n",
      "loss : 1.356961    [33408/60000]\n",
      "loss : 1.383257    [33472/60000]\n",
      "loss : 1.400974    [33792/60000]\n",
      "loss : 1.339628    [33856/60000]\n",
      "loss : 1.439243    [33920/60000]\n",
      "loss : 1.339266    [33984/60000]\n",
      "loss : 1.357808    [34304/60000]\n",
      "loss : 1.444562    [34368/60000]\n",
      "loss : 1.327213    [34432/60000]\n",
      "loss : 1.283295    [34496/60000]\n",
      "loss : 1.416854    [40960/60000]\n",
      "loss : 1.369484    [41024/60000]\n",
      "loss : 1.232116    [41088/60000]\n",
      "loss : 1.310406    [41152/60000]\n",
      "loss : 1.450047    [41472/60000]\n",
      "loss : 1.345037    [41536/60000]\n",
      "loss : 1.329082    [41600/60000]\n",
      "loss : 1.346241    [41664/60000]\n",
      "loss : 1.334191    [41984/60000]\n",
      "loss : 1.399353    [42048/60000]\n",
      "loss : 1.253126    [42112/60000]\n",
      "loss : 1.365981    [42176/60000]\n",
      "loss : 1.382598    [42496/60000]\n",
      "loss : 1.178662    [42560/60000]\n",
      "loss : 1.292464    [42624/60000]\n",
      "loss : 1.411737    [42688/60000]\n",
      "loss : 1.344529    [49152/60000]\n",
      "loss : 1.273209    [49216/60000]\n",
      "loss : 1.357558    [49280/60000]\n",
      "loss : 1.183192    [49344/60000]\n",
      "loss : 1.280936    [49664/60000]\n",
      "loss : 1.327990    [49728/60000]\n",
      "loss : 1.169858    [49792/60000]\n",
      "loss : 1.341131    [49856/60000]\n",
      "loss : 1.316523    [50176/60000]\n",
      "loss : 1.419593    [50240/60000]\n",
      "loss : 1.326957    [50304/60000]\n",
      "loss : 1.275766    [50368/60000]\n",
      "loss : 1.280646    [50688/60000]\n",
      "loss : 1.282339    [50752/60000]\n",
      "loss : 1.287483    [50816/60000]\n",
      "loss : 1.309168    [50880/60000]\n",
      "loss : 1.278168    [57344/60000]\n",
      "loss : 1.208718    [57408/60000]\n",
      "loss : 1.336196    [57472/60000]\n",
      "loss : 1.289761    [57536/60000]\n",
      "loss : 1.256212    [57856/60000]\n",
      "loss : 1.180183    [57920/60000]\n",
      "loss : 1.260468    [57984/60000]\n",
      "loss : 1.303066    [58048/60000]\n",
      "loss : 1.405956    [58368/60000]\n",
      "loss : 1.326788    [58432/60000]\n",
      "loss : 1.252993    [58496/60000]\n",
      "loss : 1.284106    [58560/60000]\n",
      "loss : 1.360171    [58880/60000]\n",
      "loss : 1.184533    [58944/60000]\n",
      "loss : 1.205504    [59008/60000]\n",
      "loss : 1.260548    [59072/60000]\n",
      "Test Error: \n",
      "Accuracy : 63.5%, Avg loss : 1.278519\n",
      "\n",
      "Epoch 4\n",
      "--------------------------\n",
      "loss : 1.353373    [    0/60000]\n",
      "loss : 1.164799    [   64/60000]\n",
      "loss : 1.238674    [  128/60000]\n",
      "loss : 1.371444    [  192/60000]\n",
      "loss : 1.248890    [  512/60000]\n",
      "loss : 1.233521    [  576/60000]\n",
      "loss : 1.153308    [  640/60000]\n",
      "loss : 1.160032    [  704/60000]\n",
      "loss : 1.277366    [ 1024/60000]\n",
      "loss : 1.323306    [ 1088/60000]\n",
      "loss : 1.239628    [ 1152/60000]\n",
      "loss : 1.376083    [ 1216/60000]\n",
      "loss : 1.149793    [ 1536/60000]\n",
      "loss : 1.200772    [ 1600/60000]\n",
      "loss : 1.140462    [ 1664/60000]\n",
      "loss : 1.270753    [ 1728/60000]\n",
      "loss : 1.307026    [ 8192/60000]\n",
      "loss : 1.150259    [ 8256/60000]\n",
      "loss : 1.254885    [ 8320/60000]\n",
      "loss : 1.279103    [ 8384/60000]\n",
      "loss : 1.194291    [ 8704/60000]\n",
      "loss : 1.248040    [ 8768/60000]\n",
      "loss : 1.309811    [ 8832/60000]\n",
      "loss : 1.348735    [ 8896/60000]\n",
      "loss : 1.170465    [ 9216/60000]\n",
      "loss : 1.167235    [ 9280/60000]\n",
      "loss : 1.187578    [ 9344/60000]\n",
      "loss : 1.262958    [ 9408/60000]\n",
      "loss : 1.236476    [ 9728/60000]\n",
      "loss : 1.338414    [ 9792/60000]\n",
      "loss : 1.218811    [ 9856/60000]\n",
      "loss : 1.317931    [ 9920/60000]\n",
      "loss : 1.162859    [16384/60000]\n",
      "loss : 1.081093    [16448/60000]\n",
      "loss : 1.205673    [16512/60000]\n",
      "loss : 1.259434    [16576/60000]\n",
      "loss : 1.197622    [16896/60000]\n",
      "loss : 1.233722    [16960/60000]\n",
      "loss : 1.244146    [17024/60000]\n",
      "loss : 1.332316    [17088/60000]\n",
      "loss : 1.265514    [17408/60000]\n",
      "loss : 1.327222    [17472/60000]\n",
      "loss : 1.160403    [17536/60000]\n",
      "loss : 1.275745    [17600/60000]\n",
      "loss : 1.248356    [17920/60000]\n",
      "loss : 1.282578    [17984/60000]\n",
      "loss : 1.400023    [18048/60000]\n",
      "loss : 1.145179    [18112/60000]\n",
      "loss : 1.115593    [24576/60000]\n",
      "loss : 1.227897    [24640/60000]\n",
      "loss : 1.230486    [24704/60000]\n",
      "loss : 1.298185    [24768/60000]\n",
      "loss : 1.248394    [25088/60000]\n",
      "loss : 1.185704    [25152/60000]\n",
      "loss : 1.081021    [25216/60000]\n",
      "loss : 1.120573    [25280/60000]\n",
      "loss : 1.159907    [25600/60000]\n",
      "loss : 1.165149    [25664/60000]\n",
      "loss : 1.211971    [25728/60000]\n",
      "loss : 1.177765    [25792/60000]\n",
      "loss : 1.217631    [26112/60000]\n",
      "loss : 1.351165    [26176/60000]\n",
      "loss : 1.200261    [26240/60000]\n",
      "loss : 1.109894    [26304/60000]\n",
      "loss : 1.202514    [32768/60000]\n",
      "loss : 1.166396    [32832/60000]\n",
      "loss : 1.185305    [32896/60000]\n",
      "loss : 1.294700    [32960/60000]\n",
      "loss : 1.089908    [33280/60000]\n",
      "loss : 1.113706    [33344/60000]\n",
      "loss : 1.125848    [33408/60000]\n",
      "loss : 1.205181    [33472/60000]\n",
      "loss : 1.186895    [33792/60000]\n",
      "loss : 1.132387    [33856/60000]\n",
      "loss : 1.246417    [33920/60000]\n",
      "loss : 1.128915    [33984/60000]\n",
      "loss : 1.164131    [34304/60000]\n",
      "loss : 1.255618    [34368/60000]\n",
      "loss : 1.077764    [34432/60000]\n",
      "loss : 1.085118    [34496/60000]\n",
      "loss : 1.209780    [40960/60000]\n",
      "loss : 1.164184    [41024/60000]\n",
      "loss : 1.013722    [41088/60000]\n",
      "loss : 1.110823    [41152/60000]\n",
      "loss : 1.272339    [41472/60000]\n",
      "loss : 1.132436    [41536/60000]\n",
      "loss : 1.139345    [41600/60000]\n",
      "loss : 1.166324    [41664/60000]\n",
      "loss : 1.152184    [41984/60000]\n",
      "loss : 1.220311    [42048/60000]\n",
      "loss : 1.041065    [42112/60000]\n",
      "loss : 1.155884    [42176/60000]\n",
      "loss : 1.192571    [42496/60000]\n",
      "loss : 0.967287    [42560/60000]\n",
      "loss : 1.100164    [42624/60000]\n",
      "loss : 1.245980    [42688/60000]\n",
      "loss : 1.136601    [49152/60000]\n",
      "loss : 1.079885    [49216/60000]\n",
      "loss : 1.171604    [49280/60000]\n",
      "loss : 0.994746    [49344/60000]\n",
      "loss : 1.080623    [49664/60000]\n",
      "loss : 1.134661    [49728/60000]\n",
      "loss : 0.973660    [49792/60000]\n",
      "loss : 1.161207    [49856/60000]\n",
      "loss : 1.125211    [50176/60000]\n",
      "loss : 1.255341    [50240/60000]\n",
      "loss : 1.134635    [50304/60000]\n",
      "loss : 1.080950    [50368/60000]\n",
      "loss : 1.085221    [50688/60000]\n",
      "loss : 1.087268    [50752/60000]\n",
      "loss : 1.096595    [50816/60000]\n",
      "loss : 1.125567    [50880/60000]\n",
      "loss : 1.101177    [57344/60000]\n",
      "loss : 1.001727    [57408/60000]\n",
      "loss : 1.166547    [57472/60000]\n",
      "loss : 1.115251    [57536/60000]\n",
      "loss : 1.075294    [57856/60000]\n",
      "loss : 0.983117    [57920/60000]\n",
      "loss : 1.084072    [57984/60000]\n",
      "loss : 1.146969    [58048/60000]\n",
      "loss : 1.265071    [58368/60000]\n",
      "loss : 1.152793    [58432/60000]\n",
      "loss : 1.044517    [58496/60000]\n",
      "loss : 1.100808    [58560/60000]\n",
      "loss : 1.202467    [58880/60000]\n",
      "loss : 0.997209    [58944/60000]\n",
      "loss : 1.027851    [59008/60000]\n",
      "loss : 1.072489    [59072/60000]\n",
      "Test Error: \n",
      "Accuracy : 64.7%, Avg loss : 1.107963\n",
      "\n",
      "Epoch 5\n",
      "--------------------------\n",
      "loss : 1.178748    [    0/60000]\n",
      "loss : 0.981469    [   64/60000]\n",
      "loss : 1.067817    [  128/60000]\n",
      "loss : 1.207985    [  192/60000]\n",
      "loss : 1.105342    [  512/60000]\n",
      "loss : 1.078836    [  576/60000]\n",
      "loss : 0.956052    [  640/60000]\n",
      "loss : 0.965620    [  704/60000]\n",
      "loss : 1.117776    [ 1024/60000]\n",
      "loss : 1.176687    [ 1088/60000]\n",
      "loss : 1.073097    [ 1152/60000]\n",
      "loss : 1.220271    [ 1216/60000]\n",
      "loss : 0.965554    [ 1536/60000]\n",
      "loss : 1.022162    [ 1600/60000]\n",
      "loss : 0.976132    [ 1664/60000]\n",
      "loss : 1.097865    [ 1728/60000]\n",
      "loss : 1.130254    [ 8192/60000]\n",
      "loss : 0.986604    [ 8256/60000]\n",
      "loss : 1.102562    [ 8320/60000]\n",
      "loss : 1.112997    [ 8384/60000]\n",
      "loss : 1.038818    [ 8704/60000]\n",
      "loss : 1.081196    [ 8768/60000]\n",
      "loss : 1.138228    [ 8832/60000]\n",
      "loss : 1.223949    [ 8896/60000]\n",
      "loss : 1.008747    [ 9216/60000]\n",
      "loss : 0.985500    [ 9280/60000]\n",
      "loss : 1.041408    [ 9344/60000]\n",
      "loss : 1.099471    [ 9408/60000]\n",
      "loss : 1.080177    [ 9728/60000]\n",
      "loss : 1.195765    [ 9792/60000]\n",
      "loss : 1.043476    [ 9856/60000]\n",
      "loss : 1.162270    [ 9920/60000]\n",
      "loss : 0.989257    [16384/60000]\n",
      "loss : 0.912545    [16448/60000]\n",
      "loss : 1.073630    [16512/60000]\n",
      "loss : 1.118629    [16576/60000]\n",
      "loss : 1.027482    [16896/60000]\n",
      "loss : 1.078975    [16960/60000]\n",
      "loss : 1.075929    [17024/60000]\n",
      "loss : 1.182418    [17088/60000]\n",
      "loss : 1.136587    [17408/60000]\n",
      "loss : 1.206418    [17472/60000]\n",
      "loss : 0.989187    [17536/60000]\n",
      "loss : 1.135907    [17600/60000]\n",
      "loss : 1.091309    [17920/60000]\n",
      "loss : 1.149796    [17984/60000]\n",
      "loss : 1.273549    [18048/60000]\n",
      "loss : 0.985942    [18112/60000]\n",
      "loss : 0.983581    [24576/60000]\n",
      "loss : 1.087983    [24640/60000]\n",
      "loss : 1.079004    [24704/60000]\n",
      "loss : 1.177601    [24768/60000]\n",
      "loss : 1.096181    [25088/60000]\n",
      "loss : 1.020332    [25152/60000]\n",
      "loss : 0.927631    [25216/60000]\n",
      "loss : 0.977778    [25280/60000]\n",
      "loss : 1.009260    [25600/60000]\n",
      "loss : 1.007633    [25664/60000]\n",
      "loss : 1.065646    [25728/60000]\n",
      "loss : 1.020100    [25792/60000]\n",
      "loss : 1.075536    [26112/60000]\n",
      "loss : 1.253481    [26176/60000]\n",
      "loss : 1.075005    [26240/60000]\n",
      "loss : 0.962255    [26304/60000]\n",
      "loss : 1.056105    [32768/60000]\n",
      "loss : 1.038245    [32832/60000]\n",
      "loss : 1.052605    [32896/60000]\n",
      "loss : 1.184430    [32960/60000]\n",
      "loss : 0.936614    [33280/60000]\n",
      "loss : 0.981427    [33344/60000]\n",
      "loss : 0.974974    [33408/60000]\n",
      "loss : 1.096996    [33472/60000]\n",
      "loss : 1.046423    [33792/60000]\n",
      "loss : 0.993663    [33856/60000]\n",
      "loss : 1.122233    [33920/60000]\n",
      "loss : 0.990960    [33984/60000]\n",
      "loss : 1.033975    [34304/60000]\n",
      "loss : 1.132388    [34368/60000]\n",
      "loss : 0.911435    [34432/60000]\n",
      "loss : 0.960034    [34496/60000]\n",
      "loss : 1.071543    [40960/60000]\n",
      "loss : 1.024423    [41024/60000]\n",
      "loss : 0.871729    [41088/60000]\n",
      "loss : 0.981881    [41152/60000]\n",
      "loss : 1.150091    [41472/60000]\n",
      "loss : 0.991860    [41536/60000]\n",
      "loss : 1.015632    [41600/60000]\n",
      "loss : 1.048066    [41664/60000]\n",
      "loss : 1.034130    [41984/60000]\n",
      "loss : 1.104725    [42048/60000]\n",
      "loss : 0.905902    [42112/60000]\n",
      "loss : 1.008169    [42176/60000]\n",
      "loss : 1.066988    [42496/60000]\n",
      "loss : 0.828885    [42560/60000]\n",
      "loss : 0.982510    [42624/60000]\n",
      "loss : 1.140720    [42688/60000]\n",
      "loss : 0.992437    [49152/60000]\n",
      "loss : 0.956738    [49216/60000]\n",
      "loss : 1.052354    [49280/60000]\n",
      "loss : 0.873532    [49344/60000]\n",
      "loss : 0.959053    [49664/60000]\n",
      "loss : 1.007133    [49728/60000]\n",
      "loss : 0.844915    [49792/60000]\n",
      "loss : 1.043086    [49856/60000]\n",
      "loss : 0.996978    [50176/60000]\n",
      "loss : 1.152619    [50240/60000]\n",
      "loss : 1.011426    [50304/60000]\n",
      "loss : 0.948215    [50368/60000]\n",
      "loss : 0.957333    [50688/60000]\n",
      "loss : 0.958633    [50752/60000]\n",
      "loss : 0.968299    [50816/60000]\n",
      "loss : 1.006368    [50880/60000]\n",
      "loss : 0.984352    [57344/60000]\n",
      "loss : 0.864032    [57408/60000]\n",
      "loss : 1.055147    [57472/60000]\n",
      "loss : 0.997375    [57536/60000]\n",
      "loss : 0.953198    [57856/60000]\n",
      "loss : 0.850491    [57920/60000]\n",
      "loss : 0.970688    [57984/60000]\n",
      "loss : 1.050155    [58048/60000]\n",
      "loss : 1.172154    [58368/60000]\n",
      "loss : 1.037821    [58432/60000]\n",
      "loss : 0.902091    [58496/60000]\n",
      "loss : 0.981919    [58560/60000]\n",
      "loss : 1.096093    [58880/60000]\n",
      "loss : 0.873500    [58944/60000]\n",
      "loss : 0.917565    [59008/60000]\n",
      "loss : 0.951510    [59072/60000]\n",
      "Test Error: \n",
      "Accuracy : 65.8%, Avg loss : 0.996008\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # 손실함수 정의\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) # 옵티마이저 정의\n",
    "epochs = 5 # 에폭 수 정의\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "    # 한 에폭에 train 1, test 1\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
